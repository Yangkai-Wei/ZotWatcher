"""æ–‡çŒ®ç›‘æµ‹æ¨¡å—"""
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime, timedelta
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import arxiv

logger = logging.getLogger("ZotWatcher.watcher")

# å»¶è¿Ÿå¯¼å…¥ crossrefï¼Œé¿å…å¯¼å…¥é”™è¯¯
try:
    from crossref.restful import Works
    CROSSREF_AVAILABLE = True
except ImportError:
    CROSSREF_AVAILABLE = False
    logger.warning("crossref-commons æœªå®‰è£…ï¼ŒCrossref åŠŸèƒ½å°†è¢«ç¦ç”¨")


class LiteratureWatcher:
    """æ–‡çŒ®ç›‘æµ‹å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.sources_config = config.get("sources", {})
        self.scoring_config = config.get("scoring", {})
        self.data_dir = Path("data")
        
        # åŠ è½½å‘é‡åŒ–æ¨¡å‹
        model_name = self.scoring_config.get("semantic", {}).get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
        logger.info(f"åŠ è½½å‘é‡åŒ–æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        # åŠ è½½ FAISS ç´¢å¼•å’Œç”»åƒ
        self._load_profile()
    
    def _load_profile(self):
        """åŠ è½½ç”¨æˆ·ç”»åƒ"""
        try:
            # åŠ è½½ FAISS ç´¢å¼•
            index_path = self.data_dir / "faiss.index"
            if index_path.exists():
                self.index = faiss.read_index(str(index_path))
                logger.info(f"å·²åŠ è½½ FAISS ç´¢å¼•ï¼ŒåŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
            else:
                self.index = None
                logger.warning("FAISS ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
            
            # åŠ è½½ç”»åƒç»Ÿè®¡ä¿¡æ¯
            profile_json_path = self.data_dir / "profile.json"
            if profile_json_path.exists():
                with open(profile_json_path, 'r', encoding='utf-8') as f:
                    self.profile_stats = json.load(f)
                logger.info(f"å·²åŠ è½½ç”»åƒç»Ÿè®¡ä¿¡æ¯")
            else:
                self.profile_stats = {}
                logger.warning("ç”»åƒç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½ç”»åƒå¤±è´¥: {e}")
            self.index = None
            self.profile_stats = {}
    
    
    def fetch_candidates(self) -> List[Dict[str, Any]]:
        """æŠ“å–å€™é€‰æ–‡ç« """
        logger.info("å¼€å§‹æŠ“å–å€™é€‰æ–‡ç« ")
        
        candidates = []
        
        # 1. Crossref
        if self.sources_config.get("sources", {}).get("crossref", {}).get("enabled"):
            crossref_articles = self._fetch_crossref()
            candidates.extend(crossref_articles)
            logger.info(f"Crossref: {len(crossref_articles)} ç¯‡")
        
        # 2. arXiv
        if self.sources_config.get("sources", {}).get("arxiv", {}).get("enabled"):
            arxiv_articles = self._fetch_arxiv()
            candidates.extend(arxiv_articles)
            logger.info(f"arXiv: {len(arxiv_articles)} ç¯‡")
        
        # 3. bioRxiv
        if self.sources_config.get("sources", {}).get("biorxiv", {}).get("enabled"):
            biorxiv_articles = self._fetch_biorxiv()
            candidates.extend(biorxiv_articles)
            logger.info(f"bioRxiv: {len(biorxiv_articles)} ç¯‡")
        
        # 4. medRxiv
        if self.sources_config.get("sources", {}).get("medrxiv", {}).get("enabled"):
            medrxiv_articles = self._fetch_medrxiv()
            candidates.extend(medrxiv_articles)
            logger.info(f"medRxiv: {len(medrxiv_articles)} ç¯‡")
        
        # 5. çƒ­é—¨æœŸåˆŠç²¾å‡†æŠ“å–
        if self.sources_config.get("top_journals", {}).get("enabled"):
            journal_articles = self._fetch_top_journals()
            candidates.extend(journal_articles)
            logger.info(f"çƒ­é—¨æœŸåˆŠ: {len(journal_articles)} ç¯‡")
        
        # å»é‡
        candidates = self._deduplicate(candidates)
        logger.info(f"å»é‡å: {len(candidates)} ç¯‡")
        
        return candidates
    
    def score_and_rank(self, candidates: List[Dict[str, Any]], top_n: int = 100) -> List[Dict[str, Any]]:
        """è¯„åˆ†å¹¶æ’åº"""
        logger.info(f"å¼€å§‹è¯„åˆ†ï¼Œç›®æ ‡æ¨è {top_n} ç¯‡")
        
        if not candidates:
            logger.warning("æ²¡æœ‰å€™é€‰æ–‡ç« ")
            return []
        
        # è®¡ç®—å„é¡¹åˆ†æ•°
        for article in candidates:
            scores = {}
            
            # 1. è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
            scores['semantic'] = self._calculate_semantic_similarity(article)
            
            # 2. æ—¶é—´è¡°å‡åˆ†æ•°
            scores['time'] = self._calculate_time_decay(article)
            
            # 3. ç™½åå•åŠ åˆ†
            scores['whitelist'] = self._calculate_whitelist_bonus(article)
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            weights = self.scoring_config.get("weights", {})
            total_score = (
                scores['semantic'] * weights.get('semantic_similarity', 0.4) +
                scores['time'] * weights.get('time_decay', 0.15) +
                scores['whitelist'] * weights.get('whitelist_bonus', 0.05)
            )
            
            article['scores'] = scores
            article['total_score'] = total_score
        
        # æ’åº
        candidates.sort(key=lambda x: x.get('total_score', 0), reverse=True)
        
        # è¿”å› top_n
        top_articles = candidates[:top_n]
        logger.info(f"è¯„åˆ†å®Œæˆï¼Œè¿”å›å‰ {len(top_articles)} ç¯‡æ–‡ç« ")
        
        return top_articles
    
    def _calculate_semantic_similarity(self, article: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        try:
            if self.index is None or self.index.ntotal == 0:
                logger.warning("FAISS ç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦")
                return 0.0
            
            # æ„å»ºæ–‡ç« æ–‡æœ¬
            title = article.get('title', '')
            abstract = article.get('abstract', '')
            text = f"{title}. {abstract}".strip()
            
            if not text:
                return 0.0
            
            # å‘é‡åŒ–
            vector = self.model.encode([text], convert_to_numpy=True)
            
            # åœ¨ FAISS ç´¢å¼•ä¸­æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
            k = min(10, self.index.ntotal)  # æœç´¢å‰ 10 ä¸ªæœ€ç›¸ä¼¼çš„
            distances, indices = self.index.search(vector.astype('float32'), k)
            
            # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆL2 è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            # ä½¿ç”¨è´ŸæŒ‡æ•°å‡½æ•°å°†è·ç¦»è½¬æ¢ä¸º [0, 1] èŒƒå›´çš„ç›¸ä¼¼åº¦
            similarities = np.exp(-distances[0] / 10.0)
            avg_similarity = np.mean(similarities)
            
            return float(avg_similarity)
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_time_decay(self, article: Dict[str, Any]) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡åˆ†æ•°"""
        try:
            # è·å–æ–‡ç« æ—¥æœŸ
            date_str = article.get('date', '')
            if not date_str:
                return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            
            # è§£ææ—¥æœŸ
            if isinstance(date_str, list):
                date_str = '-'.join(str(x) for x in date_str if x)
            
            try:
                article_date = datetime.strptime(date_str, '%Y-%m-%d')
            except:
                try:
                    article_date = datetime.strptime(date_str[:10], '%Y-%m-%d')
                except:
                    return 0.5
            
            # è®¡ç®—å¤©æ•°å·®
            days_ago = (datetime.now() - article_date).days
            
            # è·å–é…ç½®
            time_config = self.scoring_config.get("time_decay", {})
            mode = time_config.get("mode", "exponential")
            half_life = time_config.get("half_life", 3.5)
            max_days = time_config.get("max_days", 14)
            
            # è¶…è¿‡æœ€å¤§å¤©æ•°ï¼Œåˆ†æ•°ä¸º 0
            if days_ago > max_days:
                return 0.0
            
            # æŒ‡æ•°è¡°å‡
            if mode == "exponential":
                score = np.exp(-days_ago * np.log(2) / half_life)
            else:  # çº¿æ€§è¡°å‡
                daily_decay = time_config.get("daily_decay_rate", 0.1)
                score = max(0, 1 - days_ago * daily_decay)
            
            return float(score)
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ—¶é—´è¡°å‡å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_whitelist_bonus(self, article: Dict[str, Any]) -> float:
        """è®¡ç®—ç™½åå•åŠ åˆ†"""
        try:
            whitelist_config = self.scoring_config.get("whitelist", {})
            if not whitelist_config.get("enabled", False):
                return 0.0
            
            bonus_score = whitelist_config.get("bonus_score", 0.2)
            
            # æ£€æŸ¥ä½œè€…ç™½åå•
            whitelist_authors = [a.lower() for a in whitelist_config.get("authors", [])]
            article_authors = [a.lower() for a in article.get('authors', [])]
            
            for author in article_authors:
                if any(wa in author for wa in whitelist_authors):
                    return bonus_score
            
            # æ£€æŸ¥æœŸåˆŠç™½åå•
            whitelist_journals = [j.lower() for j in whitelist_config.get("journals", [])]
            article_journal = article.get('journal', '').lower()
            
            if any(wj in article_journal for wj in whitelist_journals):
                return bonus_score
            
            # æ£€æŸ¥å…³é”®è¯ç™½åå•
            whitelist_keywords = [k.lower() for k in whitelist_config.get("keywords", [])]
            title = article.get('title', '').lower()
            abstract = article.get('abstract', '').lower()
            text = f"{title} {abstract}"
            
            if any(wk in text for wk in whitelist_keywords):
                return bonus_score
            
            return 0.0
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç™½åå•åŠ åˆ†å¤±è´¥: {e}")
            return 0.0
    
    def generate_rss(self, articles: List[Dict[str, Any]], output_path: Path):
        """ç”Ÿæˆ RSS feed"""
        logger.info(f"ç”Ÿæˆ RSS feed: {output_path}")
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆ RSS feed
        current_date = datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0000")
        
        # RSS header
        rss_items = []
        for article in articles:
            title = article.get('title', 'Untitled')
            authors = article.get('authors', [])
            authors_str = ', '.join(authors[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
            if len(authors) > 3:
                authors_str += ' et al.'
            
            abstract = article.get('abstract', '')[:500]  # é™åˆ¶æ‘˜è¦é•¿åº¦
            url = article.get('url', article.get('doi', ''))
            journal = article.get('journal', '')
            date = article.get('date', '')
            score = article.get('total_score', 0)
            
            # æ ¼å¼åŒ–æ—¥æœŸä¸º RSS æ ‡å‡†æ ¼å¼
            try:
                if isinstance(date, list):
                    date_str = '-'.join(str(x) for x in date if x)
                else:
                    date_str = str(date)
                pub_date = datetime.strptime(date_str[:10], '%Y-%m-%d').strftime("%a, %d %b %Y %H:%M:%S +0000")
            except:
                pub_date = current_date
            
            description = f"""
<p><strong>Authors:</strong> {authors_str}</p>
<p><strong>Journal:</strong> {journal}</p>
<p><strong>Score:</strong> {score:.3f}</p>
<p><strong>Abstract:</strong> {abstract}...</p>
"""
            
            item = f"""
    <item>
      <title><![CDATA[{title}]]></title>
      <link>{url}</link>
      <description><![CDATA[{description}]]></description>
      <pubDate>{pub_date}</pubDate>
      <guid>{url}</guid>
    </item>"""
            
            rss_items.append(item)
        
        rss_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>ZotWatcher Recommendations</title>
    <description>Personalized academic literature recommendations based on your Zotero library</description>
    <link>https://github.com/yuzuan/ZotWatcher</link>
    <lastBuildDate>{current_date}</lastBuildDate>
    <language>en-us</language>
{chr(10).join(rss_items)}
  </channel>
</rss>"""
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(rss_content)
        
        logger.info("RSS feed ç”Ÿæˆå®Œæˆ")
    
    def generate_html_report(self, articles: List[Dict[str, Any]], output_path: Path):
        """ç”Ÿæˆ HTML æŠ¥å‘Š"""
        logger.info(f"ç”Ÿæˆ HTML æŠ¥å‘Š: {output_path}")
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆ HTML æŠ¥å‘Š
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        article_count = len(articles)
        
        # ç”Ÿæˆæ–‡ç« åˆ—è¡¨ HTML
        articles_html = []
        for idx, article in enumerate(articles, 1):
            title = article.get('title', 'Untitled')
            authors = article.get('authors', [])
            authors_str = ', '.join(authors[:5])
            if len(authors) > 5:
                authors_str += ' et al.'
            
            abstract = article.get('abstract', 'No abstract available.')
            journal = article.get('journal', 'Unknown')
            date = article.get('date', 'Unknown')
            if isinstance(date, list):
                date = '-'.join(str(x) for x in date if x)
            
            url = article.get('url', article.get('doi', '#'))
            score = article.get('total_score', 0)
            scores = article.get('scores', {})
            source = article.get('source', 'Unknown')
            
            # æ ¼å¼åŒ–åˆ†æ•°
            semantic_score = scores.get('semantic', 0)
            time_score = scores.get('time', 0)
            whitelist_score = scores.get('whitelist', 0)
            
            article_html = f"""
        <div class="article">
            <h3><a href="{url}" target="_blank">{idx}. {title}</a></h3>
            <p class="meta">
                <strong>Authors:</strong> {authors_str}<br>
                <strong>Journal:</strong> {journal} | <strong>Date:</strong> {date} | <strong>Source:</strong> {source}
            </p>
            <div class="scores">
                <span class="score-badge total">Total: {score:.3f}</span>
                <span class="score-badge">Semantic: {semantic_score:.3f}</span>
                <span class="score-badge">Time: {time_score:.3f}</span>
                <span class="score-badge">Whitelist: {whitelist_score:.3f}</span>
            </div>
            <p class="abstract">{abstract[:500]}...</p>
        </div>
"""
            articles_html.append(article_html)
        
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZotWatcher Recommendations</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }}
        
        .container {{
            max-width: 1000px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        
        h1 {{
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
        }}
        
        .header-info {{
            color: #7f8c8d;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #ecf0f1;
        }}
        
        .article {{
            margin-bottom: 30px;
            padding: 25px;
            background-color: #fff;
            border: 1px solid #e1e8ed;
            border-radius: 8px;
            transition: box-shadow 0.3s;
        }}
        
        .article:hover {{
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }}
        
        .article h3 {{
            color: #1a73e8;
            margin-bottom: 15px;
            font-size: 1.3em;
            line-height: 1.4;
        }}
        
        .article h3 a {{
            color: #1a73e8;
            text-decoration: none;
        }}
        
        .article h3 a:hover {{
            text-decoration: underline;
        }}
        
        .meta {{
            color: #5f6368;
            font-size: 0.9em;
            margin-bottom: 15px;
            line-height: 1.8;
        }}
        
        .scores {{
            margin: 15px 0;
        }}
        
        .score-badge {{
            display: inline-block;
            padding: 4px 12px;
            background-color: #e8f0fe;
            color: #1967d2;
            border-radius: 16px;
            font-size: 0.85em;
            margin-right: 8px;
            margin-bottom: 8px;
        }}
        
        .score-badge.total {{
            background-color: #34a853;
            color: white;
            font-weight: bold;
        }}
        
        .abstract {{
            color: #5f6368;
            line-height: 1.8;
            text-align: justify;
        }}
        
        .no-articles {{
            text-align: center;
            padding: 60px 20px;
            color: #7f8c8d;
        }}
        
        @media (max-width: 768px) {{
            body {{
                padding: 10px;
            }}
            
            .container {{
                padding: 20px;
            }}
            
            h1 {{
                font-size: 2em;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“š ZotWatcher Recommendations</h1>
        <div class="header-info">
            <p><strong>Generated:</strong> {current_time}</p>
            <p><strong>Total Articles:</strong> {article_count}</p>
            <p><strong>Description:</strong> Personalized academic literature recommendations based on your Zotero library</p>
        </div>
        
        {''.join(articles_html) if articles_html else '<div class="no-articles"><h2>No articles found</h2><p>Try adjusting your configuration or wait for new publications.</p></div>'}
    </div>
</body>
</html>"""
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        
        logger.info("HTML æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def push_to_zotero(self, articles: List[Dict[str, Any]]):
        """æ¨é€åˆ° Zotero"""
        logger.info("æ¨é€æ–‡ç« åˆ° Zotero")
        # TODO: å®ç° Zotero API æ¨é€
        logger.warning("push_to_zotero å°šæœªå®ç°")
    
    def _fetch_crossref(self) -> List[Dict[str, Any]]:
        """æŠ“å– Crossref"""
        if not CROSSREF_AVAILABLE:
            logger.warning("crossref-commons æœªå®‰è£…ï¼Œè·³è¿‡ Crossref æŠ“å–")
            return []
        
        try:
            crossref_config = self.sources_config.get("sources", {}).get("crossref", {})
            recent_days = self.sources_config.get("recent_days", 7)
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = datetime.now()
            start_date = end_date - timedelta(days=recent_days)
            
            logger.info(f"ä» Crossref æŠ“å– {start_date.date()} åˆ° {end_date.date()} çš„æ–‡ç« ")
            
            works = Works()
            articles = []
            
            # æŸ¥è¯¢æœ€è¿‘çš„æ–‡ç« 
            query = works.filter(
                from_pub_date=start_date.strftime('%Y-%m-%d'),
                until_pub_date=end_date.strftime('%Y-%m-%d'),
                type='journal-article'
            )
            
            # è·å–æŒ‡å®šæ•°é‡çš„ç»“æœ
            max_results = crossref_config.get('rows', 100) * crossref_config.get('max_pages', 5)
            count = 0
            
            for item in query:
                if count >= max_results:
                    break
                
                # æå–æ–‡ç« ä¿¡æ¯
                article = {
                    'title': item.get('title', [''])[0] if item.get('title') else '',
                    'abstract': item.get('abstract', ''),
                    'authors': [f"{a.get('given', '')} {a.get('family', '')}".strip() 
                               for a in item.get('author', [])],
                    'date': item.get('published-print', {}).get('date-parts', [['']])[0],
                    'journal': item.get('container-title', [''])[0] if item.get('container-title') else '',
                    'doi': item.get('DOI', ''),
                    'url': item.get('URL', ''),
                    'source': 'crossref',
                    'type': item.get('type', ''),
                }
                
                articles.append(article)
                count += 1
            
            logger.info(f"ä» Crossref è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            logger.error(f"Crossref æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _fetch_arxiv(self) -> List[Dict[str, Any]]:
        """æŠ“å– arXiv"""
        try:
            arxiv_config = self.sources_config.get("sources", {}).get("arxiv", {})
            categories = arxiv_config.get('categories', [])
            max_results = arxiv_config.get('max_results', 200)
            recent_days = self.sources_config.get("recent_days", 7)
            
            logger.info(f"ä» arXiv æŠ“å–æœ€è¿‘ {recent_days} å¤©çš„æ–‡ç« ")
            
            articles = []
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œä½¿ç”¨é€šç”¨æŸ¥è¯¢
            if not categories:
                categories = ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']
            
            # æ„å»ºæŸ¥è¯¢
            category_query = ' OR '.join([f'cat:{cat}' for cat in categories])
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            start_date = datetime.now() - timedelta(days=recent_days)
            
            # æœç´¢ arXiv
            search = arxiv.Search(
                query=category_query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            for result in search.results():
                # åªè·å–æœ€è¿‘çš„æ–‡ç« 
                if result.published.replace(tzinfo=None) < start_date:
                    continue
                
                article = {
                    'title': result.title,
                    'abstract': result.summary,
                    'authors': [author.name for author in result.authors],
                    'date': result.published.strftime('%Y-%m-%d'),
                    'journal': 'arXiv',
                    'doi': result.doi if hasattr(result, 'doi') else '',
                    'url': result.entry_id,
                    'source': 'arxiv',
                    'type': 'preprint',
                    'arxiv_id': result.entry_id.split('/')[-1],
                    'categories': [cat for cat in result.categories],
                }
                
                articles.append(article)
            
            logger.info(f"ä» arXiv è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            logger.error(f"arXiv æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _fetch_biorxiv(self) -> List[Dict[str, Any]]:
        """æŠ“å– bioRxiv"""
        # TODO: å®ç° bioRxiv API è°ƒç”¨
        logger.warning("_fetch_biorxiv å°šæœªå®ç°")
        return []
    
    def _fetch_medrxiv(self) -> List[Dict[str, Any]]:
        """æŠ“å– medRxiv"""
        # TODO: å®ç° medRxiv API è°ƒç”¨
        logger.warning("_fetch_medrxiv å°šæœªå®ç°")
        return []
    
    def _fetch_top_journals(self) -> List[Dict[str, Any]]:
        """æŠ“å–çƒ­é—¨æœŸåˆŠ"""
        # TODO: ä»ç”»åƒè¯»å–çƒ­é—¨æœŸåˆŠï¼Œç²¾å‡†æŠ“å–
        logger.warning("_fetch_top_journals å°šæœªå®ç°")
        return []
    
    def _deduplicate(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡"""
        logger.info(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹æ–‡ç« æ•°: {len(articles)}")
        
        seen_dois = set()
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            doi = article.get('doi', '').strip().lower()
            title = article.get('title', '').strip().lower()
            
            # ä¼˜å…ˆä½¿ç”¨ DOI å»é‡
            if doi and doi in seen_dois:
                continue
            
            # å¦‚æœæ²¡æœ‰ DOIï¼Œä½¿ç”¨æ ‡é¢˜å»é‡
            if not doi and title in seen_titles:
                continue
            
            # è®°å½•å¹¶æ·»åŠ 
            if doi:
                seen_dois.add(doi)
            if title:
                seen_titles.add(title)
            
            unique_articles.append(article)
        
        logger.info(f"å»é‡å®Œæˆï¼Œå‰©ä½™æ–‡ç« æ•°: {len(unique_articles)}")
        return unique_articles
